README
Comparative Analysis of RNN Architectures for IMDb Sentiment Classification

Ninad Wadode 
121317674
Homework 3
Comparative analysis of RNN/LSTM/BiLSTM architectures on the IMDb 50k sentiment dataset with controlled sweeps over activation, optimizer, sequence length, and gradient clipping. The repo supports both a pure Python CLI workflow and a Colab/Notebook workflow. All runs log metrics, aggregate summaries, and report-ready plots.
Repository structure
rnn-sentiment-comparison/
├── data/
│   ├── IMDB_Dataset.csv           # add this file (not tracked by git)
│   ├── vocab.json                 # generated by preprocess.py
│   └── dataset_stats.json         # generated by preprocess.py
├── src/
│   ├── preprocess.py              # clean, tokenize, build vocab, make padded arrays
│   ├── models.py                  # RNN, LSTM, BiLSTM definitions
│   ├── train.py                   # full factorial grid training
│   ├── evaluate.py                # metrics tables + required plots
│   └── visualize.py               # extra plots, display and save
├── results/
│   ├── metrics.csv                # all runs (appended)
│   ├── summary_rounded.csv        # compact table for report
│   ├── best_worst.json            # best/worst run metadata
│   ├── aggregates/                # aggregate CSVs for analysis
│   ├── plots/                     # png figures for the report
│   ├── preds/                     # per-run predictions (optional)
│   └── logs/                      # per-run training logs (optional)
├── requirements.txt
└── README.md

Setup
Python and OS
Python 3.10 or 3.11 recommended
Linux, macOS, or Windows


Create and activate a virtual environment
python -m venv .venv
# macOS/Linux
source .venv/bin/activate
# Windows
.\.venv\Scripts\activate

Install dependencies
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt', quiet=True)"
If you use CUDA:
Install a PyTorch build that matches your CUDA version: https://pytorch.org


Data
Download the IMDb 50k CSV (Kaggle) and place it at:
data/IMDB_Dataset.csv
The code assumes the column names review and sentiment with labels positive or negative.
Reproducibility
All scripts set:
torch.manual_seed(42)
numpy.random.seed(42)
random.seed(42)
This covers PyTorch, NumPy, and Python RNG. For exact reproducibility across hardware backends, keep the same Python and library versions and avoid non-deterministic/cuDNN autotune modes.

How to run - CLI workflow
Tip: if you are in Colab, you can run the same commands by prefixing with ! and adjusting paths from /content.
1) Preprocess
Clean text, tokenize, build top-10k vocab from train only, and save padded arrays for lengths 25, 50, 100.
python src/preprocess.py \
  --data_csv data/IMDB_Dataset.csv \
  --out_dir data \
  --seq_lengths 25 50 100
Outputs:
data/vocab.json, data/dataset_stats.json
data/X_train_len{25,50,100}.npy, data/X_test_len{25,50,100}.npy
data/y_train.npy, data/y_test.npy


2) Train the full grid
Runs all combinations of:
Architecture: rnn, lstm, bilstm
Activation: relu, tanh, sigmoid
Optimizer: adam, sgd, rmsprop
Sequence length: 25, 50, 100
Gradient clipping: off/on (max norm 1.0)
Epochs default: 5
# GPU
python src/train.py --device cuda --epochs 5
# CPU
python src/train.py --device cpu --epochs 5

Key outputs:
results/metrics.csv (appended per run)
results/logs/<run_id>.csv (loss per epoch)
results/preds/<run_id>.csv (y_true, y_prob, y_pred for test)
per-run console lines like:
 rnn-relu-adam-len25-clip0-seed42 -> acc=..., f1=..., epoch_time=...


3) Evaluate core tables and required plots
Creates summary tables and required plots (Accuracy/F1 vs length, best/worst loss curves).
python src/evaluate.py

Key outputs:
results/summary_rounded.csv
results/best_worst.json
results/aggregates/*.csv
results/plots/*.png (core plots for the report)
4) Additional visuals and inline display
Generates extra figures and, if run inside a notebook kernel, also displays them.
python src/visualize.py --base .
Outputs:
Additional plots in results/plots/
Lists and displays figures if run inside a notebook (or use %run src/visualize.py in a notebook)
How to run - Notebook workflow (Colab or Jupyter)
Upload or clone the project into the environment:
In Colab:
 %cd /content
# if zipped:
# from google.colab import files; files.upload()  # upload zip
# !unzip -q rnn-sentiment-comparison.zip
%cd /content/rnn-sentiment-comparison


Install requirements in the notebook:
 !pip install -r requirements.txt
import nltk; nltk.download('punkt', quiet=True)
Place the dataset at /content/rnn-sentiment-comparison/data/IMDB_Dataset.csv.
Run preprocessing:

 !python src/preprocess.py --data_csv data/IMDB_Dataset.csv --out_dir data --seq_lengths 25 50 100
Train:
 # T4 GPU runtime recommended: Runtime -> Change runtime type -> GPU
!python src/train.py --device cuda --epochs 5
# or CPU
# !python src/train.py --device cpu --epochs 5
Evaluate:
 !python src/evaluate.py

Visualize and display figures inline:

 # Method A: run in the same kernel so display() renders images
%run src/visualize.py --base /content/rnn-sentiment-comparison

# Method B: if you already generated plots and only want to view them inline
import glob, os
from IPython.display import Image, display
PLOT_DIR = "/content/rnn-sentiment-comparison/results/plots"
for p in sorted(glob.glob(os.path.join(PLOT_DIR, "*.png"))):
    print(os.path.basename(p))
    display(Image(filename=p))

Expected runtime and notes
On a T4-class GPU in Colab, observed average epoch times are roughly:


LSTM at seq_len=100: about 2.7–3.0 s/epoch
BiLSTM at seq_len=100: about 3.4–3.8 s/epoch
 Your times will vary by hardware and batch size.
The full grid is large (3 architectures x 3 activations x 3 optimizers x 3 lengths x 2 clipping = 162 configs x 5 epochs). If you are time constrained, start with:
--epochs 5 --device cuda for a complete but budgeted sweep
or subset the grid in train.py to focus on LSTM/BiLSTM with Adam
All scripts append to results/metrics.csv. Re-running will add rows. Delete or archive old results if you want a clean file.


Output files
Tables
results/metrics.csv — one row per config with accuracy, F1, epoch time
results/summary_rounded.csv — compact, report-ready summary
results/aggregates/*.csv — grouped means and deltas (e.g., mean F1 by model, optimizer x length, clipping deltas)
results/best_worst.json — metadata for top/bottom runs
Figures
results/plots/ includes:
f1_vs_len_by_optimizer_<arch>.png
f1_vs_len_by_activation_<arch>.png
pareto_f1_time.png
delta_f1_clipping.png
heatmap_f1_<arch>.png
Best/worst: roc_best.png, pr_best.png, cm_best.png, loss_best.png, etc.
Run artifacts 
results/preds/<run_id>.csv — y_true, y_prob, y_pred for the test set
results/logs/<run_id>.csv — per-epoch training loss


Tips and troubleshooting
If nltk tokenizer raises a resource error, run:

 python -c "import nltk; nltk.download('punkt', quiet=True)"
If cuda is not found, pass --device cpu to train.py.
Keep raw data and large arrays out of git. Use the provided .gitignore to exclude data/*.npy, IMDB_Dataset.csv, results/preds, and results/logs. Keep metrics.csv, summary_rounded.csv, aggregates, and plots for the report.
To regenerate all visuals after training, run:
 python src/evaluate.py
python src/visualize.py --base .
Citation and license
Dataset: IMDb Movie Review Dataset (Kaggle).

